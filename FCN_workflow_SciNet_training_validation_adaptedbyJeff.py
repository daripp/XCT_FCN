{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOcAS0fwumEmDrWxIciyw3w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daripp/XCT_FCN/blob/main/FCN_workflow_SciNet_training_validation_adaptedbyJeff.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEXwQdR25AAu"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "################################################\n",
        "### THIS IS THE TRAINING/VALIDATION PIPELINE ###\n",
        "################################################\n",
        "\n",
        "\n",
        "\n",
        "# # ***Welcome to the Training and Inference Pipeline ***\n",
        "# \n",
        "# # ***Step 1: Mount google drive***\n",
        "# \n",
        "# # ***Input code from google to access your drive***\n",
        "# \n",
        "# *** Consider running your instance locally. This will require modification of the file name paths, but will allow use on computers with more resources.***\n",
        "# \n",
        "# ### *To run a local instance:*\n",
        "# \n",
        "# jupyter notebook  --NotebookApp.allow_origin='https://colab.research.google.com'  --port=8080  --NotebookApp.port_retries=0\n",
        "\n",
        "\n",
        "# #**Materials**\n",
        "#   Input the material mask name and information below.\n",
        "# \n",
        "#   Specifically:\n",
        "#  \n",
        "#   **name** - The name for the material. This is pretty arbitrary, but it will be\n",
        "#   used to label output folders and images.\n",
        "#  \n",
        "#   **input_rbg_vals** - The rbg values of the material in the input mask image.\n",
        "#  \n",
        "#   **output_val** - The greyscale value of the mask when you output the images.\n",
        "#   This is arbitrary, but every material should have its own output color\n",
        "#   so they can be differentiated\n",
        "#  \n",
        "#   **confidence_threshold** - The lower this number, the more voxels will be labled a specific material. Essentially, the ML algorith outptus a confdience value  (centered on 0.5) for every voxel and every material. By default, voxels with  a confidence of 0.5 or greater are determined to be the material in question.  But we can labled voxles with a lower condience level by changing this  parameter\n",
        "#   \n",
        "#   **training_image_directory /training_mask_directory**: Input the directory where your training images and masks are located.\n",
        "# \n",
        "#   **validation_fraction**: Input the fraction of images you want to validate your model during training. These are not a independent validation, but are part of the training process.\n",
        "# \n",
        "#   **num_models**: Enter the number of models you want to iteratively train. Because these are statistical models, the performance of any given model will vary. Training more models will allow you to select the model that best fits your data.\n",
        "#   \n",
        "#   **num_epochs**: Enter number of epochs that you want to use to train your model. More is generally better, but takes more time.\n",
        "# \n",
        "#   **batch_size**: Input your batch size. Larger batch sizes allow for faster training, but take up more VRAM. If you are running out of VRAM during training, decrease your batch size.\n",
        "# \n",
        "#   **scale**: Input how you want your images scaled during model training and inference. When the scale is 1, your images will be used at full size for training. When the scale is less than 1, your images will be downsized according to the scale you set for training and inference, decreasing VRAM usage. If you run out of VRAM during training, consider rescaling your images.\n",
        "# \n",
        "#   **models_directory**: Directory where your models are saved.\n",
        "# \n",
        "#   **model_group**: Name for the group models you iteratively generate.\n",
        "# \n",
        "#   **current_model_name**: Name for each individual model you generate; will automatically be labeled 1 through n for the number of models you specify above.\n",
        "# \n",
        "#   **val_images/val_masks**: Input the directory where your independent validation images and masks are located. These images are not used for training and are used as an independent validation of your model.\n",
        "# \n",
        "#   **csv_directory**: Directory where a CSV file of your validation results will be saved.\n",
        "# \n",
        "#   **inference_directory**: Directory where the images you want analyzed are located.\n",
        "# \n",
        "#   **output_directory**: Directory where you want your analysis results to be saved.\n",
        "# \n",
        "# \n",
        "\n",
        "# In[ ]\n",
        "\n",
        "# Import datetime for saving the date\n",
        "from datetime import datetime\n",
        "# Create a date string\n",
        "ds = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "\n",
        "class Material:\n",
        " \n",
        "  def __init__(self, name, input_rgb_vals, output_val, confidence_threshold=0):\n",
        "    self.name = name\n",
        "    self.input_rgb_vals = input_rgb_vals\n",
        "    self.output_val = output_val\n",
        "    self.confidence_threshold = confidence_threshold\n",
        "\n",
        "#Creating a list of materials so we can iterate through it\n",
        "materials = [\n",
        "             Material(\"notberry\", [0,0,0], 1, 0.5),\n",
        "             Material(\"berry\", [255,255,255], 100, 0.5),\n",
        "             ]\n",
        "\n",
        "# Boolean whether to retrain models or use current models\n",
        "new_training = False\n",
        "\n",
        "# Project directory\n",
        "# IMPORTANT - ALL DIRECTORIES NEED TO END IN A /\n",
        "proj_dir = \"/path/to/project/directory/\"\n",
        " \n",
        "num_materials =len(materials)\n",
        "\n",
        "#Various input/output directories\n",
        "# IMPORTANT: END EACH DIRECTORY PATH WITH A \"/\"\n",
        "training_image_directory = proj_dir + \"train/images/\"\n",
        "training_mask_directory = proj_dir + \"train/masks/\"\n",
        "\n",
        "#Fraction of total annotations you want to leave for validating the model.\n",
        "validation_fraction=0.2\n",
        "\n",
        "#Model Performance varies, make multiple models to have the best chance at success.\n",
        "num_models=7\n",
        "\n",
        "#Model Performance improves with increasing epochs, to a point.\n",
        "num_epochs=70\n",
        "\n",
        "# \"\"\"Increasing batch size increase model training speed, but also eats up VRAM on the GPU. Find a balance between scale and batch size\n",
        "# that best suits your needs\"\"\"\n",
        "batch_size=3\n",
        "\n",
        "#Decrease scale to decrease VRAM usage; if you run out of VRAM during traing, restart your runtime and down scale your images\n",
        "scale=0.8\n",
        "\n",
        "#Input model directory\n",
        "# IMPORTANT: END EACH DIRECTORY PATH WITH A \"/\"\n",
        "models_directory = proj_dir + \"best_models/\"\n",
        "\n",
        "#Input the name you want to use for your group of models\n",
        "model_group='model_group_name/'\n",
        "\n",
        "## THIS IS NOT A DIRECTORY; DO NOT ADD TRAILING \"/\"\n",
        "# Model name is based on the current date; so multiple runs are not clobbered\n",
        "current_model_name = \"model_name_\" + ds + \"_model\"\n",
        "\n",
        "# \"\"\"Hold images/annotations in reserve to test your model performance. Use this metric to decide which model you want to use \n",
        "# for your data analysis\"\"\"\n",
        "# IMPORTANT: END EACH DIRECTORY PATH WITH A \"/\"\n",
        "test_images = proj_dir + \"test/images/\"\n",
        "test_masks= proj_dir + \"test/masks/\"\n",
        "csv_directory =  proj_dir + model_group.replace(\"/\", \"\") + \".csv\"\n",
        "\n",
        "#Input the directory of the data you want to segment here.\n",
        "inference_directory= proj_dir + 'inferenceImages/'\n",
        "\n",
        "#Input the 5 alpha-numeric characters proceding the file number of your images\n",
        "  #EX. Jmic3111_S0_GRID image_0.tif ----->mage_\n",
        "proceeding=\"mage_\"\n",
        "#Input the 4 or mor alpha-numeric characters following the file number\n",
        "  #EX. Jmic3111_S0_GRID image_0.tif ----->.tif\n",
        "following=\".jpg\"\n",
        "\n",
        "output_directory = proj_dir + model_group + 'watershed_adj/'\n",
        "\n",
        "\n",
        "## Write variables to a file ##\n",
        "\n",
        "# # List all objects\n",
        "# objects = locals()\n",
        "# \n",
        "# ## Write all of the above parameters to a python script that will be imported at later stages\n",
        "# param_filename = proj_dir + \"fcn_workflow_parameters_\" + ds + \".py\"\n",
        "#\n",
        "#\n",
        "# # Open a file\n",
        "# handle = open(param_filename, \"w\")\n",
        "#\n",
        "# # Write all the parameters to this file\n",
        "# for key in objects:\n",
        "#  value = objects[key]\n",
        "#  # If the object value is one of the following, print it\n",
        "#  if (type(value).__name__ in [\"str\", \"int\", \"list\"]):\n",
        "#    if type(value).__name__ == \"int\":\n",
        "#      handle.write(key + ' = ' + str(value) + '\\n')\n",
        "#    else:\n",
        "#      handle.write(key + ' = \"' + str(value) + '\"\\n')\n",
        "#\n",
        "# # Close the file\n",
        "# handle.close()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "################################################################################\n",
        "### DO NOT EDIT BELOW THIS LINE ################################################\n",
        "################################################################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html    \n",
        "\n",
        "\n",
        "# #**Parameter Loading**\n",
        "\n",
        "#Code Box 2\n",
        "from os.path import splitext\n",
        "from os import listdir\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import logging\n",
        "from PIL import Image\n",
        "import random\n",
        "#import scipy.ndimage as ndi\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from scipy.ndimage import morphology\n",
        "from torch.utils.data import DataLoader, random_split\n",
        " \n",
        "class BasicDataset(Dataset):\n",
        "    def __init__(self, imgs_dir, masks_dir, scale=scale, transform=False):\n",
        "        self.imgs_dir = imgs_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.scale = scale\n",
        "        self.transform=transform\n",
        "        self.t_list=A.Compose([A.HorizontalFlip(p=0.4),A.VerticalFlip(p=0.4), A.Rotate(limit=(-50, 50), p=0.4),])\n",
        "        self.means=[0]\n",
        "        self.stds=[1]\n",
        "\n",
        "        \n",
        "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
        " \n",
        "        self.ids = [splitext(file)[0] for file in listdir(imgs_dir)\n",
        "                    if not file.startswith('.')]\n",
        "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        " \n",
        " \n",
        "    @classmethod\n",
        "    def mask_preprocess(cls, pil_img, scale):\n",
        "        w, h = pil_img.size\n",
        "        newW, newH = int(scale * w), int(scale * h)\n",
        "        assert newW > 0 and newH > 0, 'Scale is too small'\n",
        "        pil_img = pil_img.resize((newW, newH))\n",
        " \n",
        "        img_nd = np.array(pil_img)\n",
        " \n",
        "        if len(img_nd.shape) == 2:\n",
        "            img_nd = np.expand_dims(img_nd, axis=2)\n",
        " \n",
        "       \n",
        "        return img_nd\n",
        "    \n",
        " \n",
        "        \n",
        "    def img_preprocess(cls, pil_img, scale):\n",
        "        w, h = pil_img.size\n",
        "        newW, newH = int(scale * w), int(scale * h)\n",
        "        assert newW > 0 and newH > 0, 'Scale is too small'\n",
        "        pil_img = pil_img.resize((newW, newH))\n",
        " \n",
        "        img_nd = np.array(pil_img)\n",
        " \n",
        "        if len(img_nd.shape) == 2:\n",
        "            img_nd = np.expand_dims(img_nd, axis=2)\n",
        " \n",
        "       \n",
        " \n",
        "        return img_nd\n",
        " \n",
        "    def __getitem__(self, i):\n",
        "        idx = self.ids[i]\n",
        "        mask_file = glob(self.masks_dir + idx + '*')\n",
        "        img_file = glob(self.imgs_dir + idx + '*')\n",
        " \n",
        "        assert len(mask_file) == 1,             f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
        "        assert len(img_file) == 1,             f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
        "        mask = Image.open(mask_file[0])\n",
        "        img = Image.open(img_file[0])\n",
        " \n",
        "  \n",
        "        \n",
        " \n",
        "        \n",
        "        #Reshapes from 1 channel to 3 channels in grayscale\n",
        "        img = self.img_preprocess(img, self.scale)\n",
        "        mask = self.mask_preprocess(mask, self.scale)\n",
        "        new_image=np.zeros((img.shape[0],img.shape[1],3))\n",
        "        new_image[:,:,0]=img[:,:,0]\n",
        "        new_image[:,:,1]=img[:,:,0]\n",
        "        new_image[:,:,2]=img[:,:,0]\n",
        "        \n",
        " \n",
        " \n",
        " \n",
        "        img=new_image\n",
        " \n",
        "        new_mask = np.zeros((num_materials,img.shape[0],img.shape[1]))\n",
        "        # print(mask.shape)       \n",
        "        for i, mat in enumerate(materials):\n",
        "          # plt.imshow(mask[:,:,0])\n",
        "          # plt.show()\n",
        "          indices = np.all(mask == mat.input_rgb_vals, axis=-1)\n",
        "          new_mask[i,:,:][indices] = 1\n",
        " \n",
        "        mask = new_mask\n",
        "  \n",
        "        # plt.imshow(mask[1,:,:])\n",
        "        # i=6\n",
        "        # for i in range(len(mask)):\n",
        "        #   plt.imshow(mask[i,:,:])\n",
        "        #   plt.show()\n",
        "        \n",
        "        if img.max() > 1:\n",
        "            img = img / 255\n",
        " \n",
        "       \n",
        " \n",
        "        \n",
        "        if self.transform:\n",
        "            augmented=self.t_list(image=img, masks=mask)\n",
        "            img=augmented[\"image\"]\n",
        "            mask=augmented[\"masks\"]\n",
        "            \n",
        " \n",
        "        \n",
        " \n",
        "        img = img.transpose((2, 0, 1))\n",
        "        \n",
        "        mask=np.array(mask)\n",
        "        \n",
        "        \n",
        " \n",
        "        \n",
        " \n",
        "        img=torch.from_numpy(img)\n",
        "        mask=torch.from_numpy(mask)\n",
        "        \n",
        "        img=transforms.Normalize(mean=self.means, std=self.stds)(img)\n",
        "        return img, mask\n",
        "        \n",
        "        \n",
        "dataset = BasicDataset(training_image_directory, training_mask_directory, scale=scale, transform=False)\n",
        " \n",
        "#!!!!!!!!!!!!!!!!!!!!!!!!!!Set batch size here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "# train, val=trainval_split(dataset, val_fraction=0.5)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)#, collate_fn=pad_collate)\n",
        "#val_loader = DataLoader(val, batch_size=3, shuffle=False, num_workers=0, pin_memory=True)#, collate_fn=pad_collate)\n",
        "nimages = 0\n",
        "mean = 0.\n",
        "std = 0.\n",
        "for batch, _ in train_loader:\n",
        "    # Rearrange batch to be the shape of [B, C, W * H]\n",
        "    batch = batch.view(batch.size(0), batch.size(1), -1)\n",
        "    # Update total number of images\n",
        "    nimages += batch.size(0)\n",
        "    # Compute mean and std here\n",
        "    mean += batch.mean(2).sum(0) \n",
        "    std += batch.std(2).sum(0)\n",
        " \n",
        "# Final step\n",
        "mean /= nimages\n",
        "std /= nimages\n",
        " \n",
        "print(mean)\n",
        "print(std)\n",
        "\n",
        "dataset.means=mean\n",
        "dataset.stds=std \n",
        "\n",
        "nimages = 0\n",
        "newmean = 0.\n",
        "newstd = 0.\n",
        "for batch, _ in train_loader:\n",
        "    # Rearrange batch to be the shape of [B, C, W * H]\n",
        "    batch = batch.view(batch.size(0), batch.size(1), -1)\n",
        "    # Update total number of images\n",
        "    nimages += batch.size(0)\n",
        "    # Compute mean and std here\n",
        "    newmean += batch.mean(2).sum(0) \n",
        "    newstd += batch.std(2).sum(0)\n",
        " \n",
        "# Final step\n",
        "newmean /= nimages\n",
        "newstd /= nimages\n",
        " \n",
        "print(newmean)\n",
        "print(newstd)\n",
        "\n",
        "\n",
        "# Now you are ready to train your model in code block 4. \n",
        "# If you run out of VRAM, restart the runtime, reload google drive, and try again. Also consider rescaling your images or decreasing your batch size.\n",
        "# \n",
        "\n",
        "# #**Model** **Training**\n",
        "# Please do not alter this code.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "## Proceed with model training only if the indicator above is True\n",
        "if new_training:\n",
        "\n",
        "  #For loop for FCN model training Cell Code Box 4\n",
        "  #!cd \"drive/My Drive/Colab Notebooks\"\n",
        "  # Semantic Segmentation and Data Extraction in Pytorch Using FCN by Pranav Raja and a tiny bit by Devin Rippner (Plant AI and BioPhysics Lab)\n",
        "  # a work in progress, works well overall but need mroe people to look at it and identify bugs\n",
        "  #%%\n",
        "   \n",
        "  import torchvision\n",
        "  from torchvision.models.segmentation.fcn import FCNHead\n",
        "  from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "  from torch.utils.data import DataLoader, random_split\n",
        "  import torch\n",
        "  # from torch._six import container_abcs, string_classes, int_classes\n",
        "  import torchvision.transforms as T\n",
        "  import matplotlib.pyplot as plt\n",
        "  import torch.nn as nn\n",
        "  import os\n",
        "  import psutil\n",
        "  import gc\n",
        "  import random\n",
        "   \n",
        "  dir_checkpoint = models_directory\n",
        "   \n",
        "   \n",
        "  model_group=model_group\n",
        "  num_models=num_models\n",
        "  # Create model group and name paths\n",
        "  model_group_dir1 = os.path.join(dir_checkpoint, model_group)\n",
        "  if not os.path.exists(model_group_dir1):\n",
        "    os.mkdir(model_group_dir1)\n",
        "  \n",
        "  seed=0\n",
        "  torch.manual_seed(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  for i in range(num_models):\n",
        "    #!!!!!!! Here we pull in a pretrained FCN on torch and we replace the output layer since we have six classes rather than 21!!!!!!!!\n",
        "    num_classes=num_materials\n",
        "    model=torchvision.models.segmentation.fcn_resnet101(pretrained=True, progress=True)\n",
        "    # model.backbone.conv1=nn.Sequential(nn.Conv2d(1,3, (1,1), (1,1), (0,0), bias=False), model.backbone.conv1)\n",
        "    model.classifier=FCNHead(2048, num_classes)\n",
        "    \n",
        "    def trainval_split(dataset, val_fraction=0.5):\n",
        "   \n",
        "      validation_size = int(len(dataset) * val_fraction)\n",
        "      train_size = len(dataset) - validation_size\n",
        "      # print(validation_size)\n",
        "      # print(train_size)\n",
        "      # print(len(dataset))\n",
        "      # print(dataset.dataset_size)\n",
        "      train, val = torch.utils.data.random_split(dataset, [train_size, validation_size], generator=torch.Generator().manual_seed(i))\n",
        "   \n",
        "      return train, val\n",
        "   \n",
        "   \n",
        "   \n",
        "    dataset= BasicDataset(training_image_directory, training_mask_directory, scale=scale, transform=True)\n",
        "    dataset_train, dataset_val=trainval_split(dataset, val_fraction=validation_fraction)\n",
        "    #!!!!!select folders for the images and masks associated with training and validation here. Also specify image scaling factor here!!!!!!!!!!!!!!!!\n",
        "    # dataset_train = BasicDataset(training_image_directory, training_mask_directory, 1, transform=True)\n",
        "    # dataset_val = BasicDataset(training_image_directory, training_mask_directory, 1, transform=False)\n",
        "    # dataset_train = BasicDataset(\"drive/My Drive/FCN WORKFLOW PAPER/train/image_/\", \"drive/My Drive/FCN WORKFLOW PAPER/train/mask_edited2/\", 1, transform=True)\n",
        "    # dataset_val = BasicDataset(\"drive/My Drive/FCN WORKFLOW PAPER/test/image_/\", \"drive/My Drive/FCN WORKFLOW PAPER/test/mask_edited2/\", 1, transform=False)\n",
        "    \n",
        "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!Specify Batch Size Here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)#, collate_fn=pad_collate)\n",
        "    val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)#, collate_fn=pad_collate)\n",
        "   \n",
        "   \n",
        "    #%%\n",
        "   \n",
        "    # this is the train code \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!! Input epochs here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    num_epochs=num_epochs\n",
        "    # read up on optimizers but Adam should work for now, if you get good results with Adam then you can try SGD (it's harder to tune but usually converges better)\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "   \n",
        "    #just initializing a value called best_loss\n",
        "    best_loss=999\n",
        "   \n",
        "    # choose a loss function\n",
        "    # criterion=nn.CrossEntropyLoss()\n",
        "    #criterion=nn.BCELoss().cuda()\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # class diceloss(nn.Module):\n",
        "    #     def __init__(self, epsilon):\n",
        "    #         # super(diceloss, self).init()\n",
        "    #         super(diceloss, self).__init__()\n",
        "    #         self.sigmoid=nn.Sigmoid()\n",
        "    #         self.epsilon=epsilon\n",
        "    #         # print('HI')\n",
        "    #     def forward(self, pred, target):\n",
        "    #         if target.size() != pred.size():\n",
        "    #             raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), pred.size()))\n",
        "    #         pred=self.sigmoid(pred)\n",
        "    #         tp=torch.sum(target*pred, (1,2,3))\n",
        "    #         fp=torch.sum((1-target)*pred, (1,2,3))\n",
        "    #         fn=torch.sum(target*(1-pred), (1,2,3))\n",
        "    #         # precision=tp/(tp+fp)\n",
        "    #         # recall=tp/(tp+fn)\n",
        "    #         f1=(tp)/(tp+self.epsilon+0.5*(fp+fn))\n",
        "    #         # print(f1)\n",
        "    #         return 1-torch.mean(f1)\n",
        "    # criterion=diceloss(epsilon=epsilon)\n",
        "    #\n",
        "    # model.train()\n",
        "    # model.train()\n",
        "    # \n",
        "    #this is the train loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(psutil.virtual_memory().percent)\n",
        "        print('Epoch: ', str(epoch))\n",
        "      #add back if doing fractional training\n",
        "        train_loader.dataset.dataset.transform=True\n",
        "        model.train()\n",
        "        for images, masks in train_loader:\n",
        "   \n",
        "            images = images.to(device=device, dtype=torch.float32)\n",
        "            masks = masks.to(device=device, dtype=torch.float32)\n",
        "   \n",
        "            #forward pass\n",
        "            preds=model(images)['out'].cuda()\n",
        "          \n",
        "            #compute loss\n",
        "            loss=criterion(preds, masks)\n",
        "          \n",
        "            #reset the optimizer gradients to 0\n",
        "            optimizer.zero_grad()\n",
        "   \n",
        "            #backward pass (compute gradients)\n",
        "            loss.backward()\n",
        "   \n",
        "            #use the computed gradients to update model weights\n",
        "            optimizer.step()\n",
        "   \n",
        "            print('Train loss: '+str(loss.to('cpu').detach()))\n",
        "        # model.eval()\n",
        "        #add back if doing fractional training\n",
        "        val_loader.dataset.dataset.transform=False\n",
        "        current_loss=0\n",
        "        \n",
        "        #test on val set and save the best checkpoint\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          for images, masks in val_loader:\n",
        "              images = images.to(device=device, dtype=torch.float32)\n",
        "              masks = masks.to(device=device, dtype=torch.float32)\n",
        "              preds=model(images)['out'].cuda()\n",
        "              # print(preds)\n",
        "              # print(masks)\n",
        "              loss=criterion(preds, masks)\n",
        "              #print('hi')\n",
        "              current_loss+=loss.to('cpu').detach()\n",
        "              del images, masks, preds, loss\n",
        "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!Re-name model here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!        \n",
        "        if best_loss>current_loss:\n",
        "            best_loss=current_loss\n",
        "            print('Best Model Saved!, loss: '+ str(best_loss))\n",
        "            torch.save(model.state_dict(), dir_checkpoint+model_group + current_model_name+str(i+1)+\".pth\")\n",
        "        else:\n",
        "            print('Model is bad!, Current loss: '+ str(current_loss) + ' Best loss: '+str(best_loss))\n",
        "        print('\\n')\n",
        "        \n",
        "\n",
        "# If no retraining, adjust the current model name to reflect the most recent models\n",
        "else:\n",
        "  import re\n",
        "  import os\n",
        "  models_dir = os.path.join(models_directory, model_group)\n",
        "  # List the trained models\n",
        "  trained_models = [x for x in os.listdir(models_dir) if \".pth\" in x]\n",
        "  # Cut the #.pth from the end of each file; find the unique names\n",
        "  unique_model_name = list(set([re.sub(\"\\d+.pth\", \"\", x) for x in trained_models]))\n",
        "  # Sort\n",
        "  unique_model_name.sort()\n",
        "  # Get the last item (this is the most recent)\n",
        "  current_model_name = unique_model_name[-1]\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\"\"\"# **Validation**\n",
        "Please do not alter this code\n",
        "\"\"\"\n",
        "\n",
        "# Recalculate the number of models\n",
        "import os\n",
        "models_dir = os.path.join(models_directory, model_group)\n",
        "num_models = len([x for x in os.listdir(models_dir) if current_model_name in x and \".pth\" in x])\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch\n",
        "from torchvision.models.segmentation.fcn import FCNHead\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "#from statistics import mean\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# interim_list=[]\n",
        "modeldata = pd.DataFrame(columns=[\"name\", \"precision\", \"recall\", \"accuracy\", \"f1\"])\n",
        "# for mat in enumerate(materials):\n",
        "#   modeldata=pd.DataFrame(columns=['model_group',mat.name + \" precision\",mat.name + \" recall\",mat.name + \" accuracy\",mat.name + \" f1\"])\n",
        "# num_models=num_models\n",
        " \n",
        "for s in range(num_models):\n",
        "  # model=torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True, progress=True)\n",
        " \n",
        " \n",
        "  model=torchvision.models.segmentation.fcn_resnet101(pretrained=False)\n",
        "  # model.backbone.conv1=nn.Sequential(nn.Conv2d(1,3, (1,1), (1,1), (0,0), bias=False), model.backbone.conv1)\n",
        "  #!!!!!!!!!!!!!!!!!Specify Layer # here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "  model.classifier=FCNHead(2048, num_materials)\n",
        "  # model.classifier=DeepLabHead(2048, 6)\n",
        "  device = torch.device('cuda')\n",
        " \n",
        "  outputs=[]\n",
        "  model.to(device)\n",
        " \n",
        "  #!!!!!!!!!!!!!!!!!!!!!Select Correct Model from the best models directory!!!!!!!!!!!!!!!!!!!!!!!!!1\n",
        " \n",
        "  model.load_state_dict(torch.load(models_directory+model_group + current_model_name+str(s+1)+\".pth\"), strict=False)\n",
        " \n",
        " \n",
        "  model.train()\n",
        " \n",
        "  dataset_val = BasicDataset(test_images, test_masks, scale=scale, transform=False)\n",
        "  val_loader = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)#, collate_fn=pad_collate)\n",
        " \n",
        "  prop_list = []\n",
        "  for mat in materials:\n",
        "    prop_list.append([[],[],[],[]])\n",
        " \n",
        "  for images, target in val_loader:\n",
        "    images = images.to(device=device, dtype=torch.float32)\n",
        "    target = target.to(device=device, dtype=torch.float32)\n",
        " \n",
        "    with torch.no_grad():\n",
        "      pred=model(images)['out'].cuda()\n",
        "      pred=nn.Sigmoid()(pred)\n",
        "    \n",
        "    for i, mat in enumerate(materials):\n",
        "      material_target=target[:,i,:,:]\n",
        "      material_pred = pred[:, i, :, :]\n",
        "      material_pred[material_pred >=mat.confidence_threshold] = 1\n",
        "      material_pred[material_pred <=mat.confidence_threshold] = 0\n",
        "      pred[:, i, :, :]=material_pred\n",
        " \n",
        "      material_tp=torch.sum(material_target*material_pred, (1,2))\n",
        "      material_fp=torch.sum((1-material_target)*material_pred, (1,2))\n",
        "      material_fn=torch.sum(material_target*(1-material_pred), (1,2))\n",
        "      material_tn=torch.sum((1-material_target)*(1-material_pred), (1,2))\n",
        " \n",
        "     \n",
        " \n",
        "      material_precision=torch.mean((material_tp+0.000000001)/(material_tp+material_fp+0.000000001))\n",
        "      material_recall=torch.mean((material_tp+0.000000001)/(material_tp+material_fn+0.000000001))\n",
        "      material_accuracy=torch.mean((material_tp+material_tn+0.000000001)/(material_tp+material_tn+material_fp+material_fn+0.000000001))\n",
        "      material_f1=torch.mean(((material_tp+0.000000001))/(material_tp++0.000000001+0.5*(material_fp+material_fn)))\n",
        " \n",
        "    \n",
        "      prop_list[i][0].append(material_precision.cpu().detach().numpy())\n",
        "      prop_list[i][1].append(material_recall.cpu().detach().numpy())\n",
        "      prop_list[i][2].append(material_accuracy.cpu().detach().numpy())\n",
        "      prop_list[i][3].append(material_f1.cpu().detach().numpy())\n",
        " \n",
        "          \n",
        " \n",
        " \n",
        " \n",
        "  # print(current_model_name+str(s+1))\n",
        "  model_name=current_model_name\n",
        "  model_number=(str(s+1))\n",
        "  print(model_name)\n",
        " \n",
        "  #printing with pandas\n",
        "  properties = {\"name\" : [mat.name for mat in materials],\n",
        "                \"precision\" : [str(np.mean(prop_list[i][0])) for i in range(num_materials)],\n",
        "                \"recall\" : [str(np.mean(prop_list[i][1])) for i in range(num_materials)],\n",
        "                \"accuracy\" : [str(np.mean(prop_list[i][2])) for i in range(num_materials)],\n",
        "                \"f1\" : [str(np.mean(prop_list[i][3])) for i in range(num_materials)]}\n",
        "  df = pd.DataFrame(properties, columns = [\"name\", \"precision\", \"recall\", \"accuracy\", \"f1\"])\n",
        "  df=pd.concat([df, pd.DataFrame(columns=[\"model number\",\"model name\"])])\n",
        "  df[[\"model number\",\"model name\"]]=[model_number, model_name]\n",
        "  # display(df)\n",
        "  \n",
        "  modeldata=modeldata.append([df], ignore_index=True)\n",
        "\n",
        "\n",
        " \n",
        "#   for i, mat in enumerate(materials):\n",
        "#     precision_final = np.mean(prop_list[i][0])\n",
        "#     print(mat.name + \" precision: \" + str(precision_final))\n",
        "#     recall_final = np.mean(prop_list[i][1])\n",
        "#     print(mat.name + \" recall: \" + str(recall_final))\n",
        "#     accuracy_final = np.mean(prop_list[i][2])\n",
        "#     print(mat.name + \" accuracy: \" + str(accuracy_final))\n",
        "#     f1_final = np.mean(prop_list[i][3])\n",
        "#     print(mat.name + \" f1: \" + str(f1_final))\n",
        "#     # modeldata1=modeldata.append({'name': mat.name, mat.name + \" precision\": precision_final, mat.name + \" recall\": recall_final, mat.name + \" accuracy\": accuracy_final, mat.name + \" f1\": f1_final}, ignore_index=True)\n",
        "#     # model_data=modeldata.append(modeldata1)\n",
        "# # model_data=df.append(interim_list)\n",
        "# print(modeldata)\n",
        "# md=pd.concat([modeldata, pd.DataFrame(columns=[\"model name\"])])\n",
        "# md[[\"model name\"]]=[current_model_name]\n",
        "# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "# display(modeldata)\n",
        "\n",
        "#   display(modeldata)\n",
        "\n",
        "\"\"\"#**Save Validation CSV**\n",
        "Please do not alter this code\n",
        "\"\"\"\n",
        "# display(modeldata)\n",
        "modeldata.to_csv(csv_directory)"
      ]
    }
  ]
}